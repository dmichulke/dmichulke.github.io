<?xml version='1.0' encoding='UTF-8'?>
<rss version='2.0' xmlns:atom='http://www.w3.org/2005/Atom'>
<channel>
<atom:link href='https://dmichulke.github.io/' rel='self' type='application/rss+xml'/>
<title>
Strength in Numbers
</title>
<link>
https://dmichulke.github.io/
</link>
<description>
Leveraging Your Data To Solve Your Problems
</description>
<lastBuildDate>
Wed, 11 Apr 2018 10:46:20 +0200
</lastBuildDate>
<generator>
clj-rss
</generator>
<item>
<guid>
https://dmichulke.github.io/posts-output/2018-04-03-counting-trees/
</guid>
<link>
https://dmichulke.github.io/posts-output/2018-04-03-counting-trees/
</link>
<title>
Automatic Counting of Trees in Drone Images
</title>
<description>
&lt;p&gt;Counting trees may not exactly sound like interesting or lucrative business. Yet, with a &lt;a href='http://www.fao.org/forestry/statistics/80938/en/'&gt;global market size of 227 billion $&lt;/a&gt; there are obviously many more people active in the forestry sector than anyone not belonging to the sector would initially assume. &lt;/p&gt;&lt;p&gt;Forestry plantations require tracking the development of its trees in order to identify health and growth problems in the trees and to project the optimal timing of a harvest and its monetary value.&lt;/p&gt;&lt;p&gt;Typically, this is done by analyzing various small samples throughout plantation and inferring from that the total volume of lumber. Analyzing here means: Count trees, measure thickness of the trees, and observe any anomalies. &lt;/p&gt;&lt;p&gt;In the project at hand, we were analyzing an area of a few hundred hectars of pines (&lt;a href='https://en.wikipedia.org/wiki/Pinus_taeda'&gt;pinus taeda&lt;/a&gt;) in the state of Santa Catarina in South Brazil. &lt;/p&gt;&lt;p&gt;The analysis, carried out by my cousin's company (&lt;a href='http://www.supergeo.com.br/'&gt;SuperGeo Engenharia e Topografia&lt;/a&gt;) found each hectar to be worth around 6000$, after discounting costs for logging and transport. The analysis took a dozen men-days due to the size of the area.&lt;/p&gt;&lt;p&gt;However, this time we also used drone footage and applied Computer Vision and Image Recognition algorithms to do the counting for us. The potential saving in time and work is enormous with costs expected to be at most of 50% of the original approach, going possibly down to 80% for big projects.&lt;/p&gt;&lt;p&gt;The aim of this post is to give you an insight into where we currently stand in this endeavour.&lt;/p&gt;&lt;h3 id=&quot;from&amp;#95;geo-data&amp;#95;to&amp;#95;a&amp;#95;raw&amp;#95;image&quot;&gt;From Geo-Data to a Raw Image&lt;/h3&gt;&lt;p&gt;As inputs we received the Geo-Data of the area in question. So we went there and sent out a drone to take pictures of the area in question and weave those pictures together to one big raw image. The pictures taken by the drone camera had a resolution where each pixel is equivalent to an approximate area of 10x10 cmÂ². f This means that a hectar translates to the size of a standard cell phone picture (1 million pixels) and an area of 3x3 kilometers (900 hectars) gives you a picture the size of a gigabyte. This image is big enough to take quite a few seconds to load on a modern computer and in this case already too big to be processed by my standard image viewing programs: For images bigger than 2GB my applications could not read the pictures anymore, I suppose that's because they don't expect (and were never tested with) &quot;photos&quot; of that size.&lt;/p&gt;&lt;p&gt;&lt;figure&gt;   &lt;img src=&quot;img/overview.jpg&quot; alt=&quot;Overview of the area&quot;/&gt;   &lt;figcaption&gt;Overview of the area&lt;/figcaption&gt; &lt;/figure&gt;&lt;/p&gt;&lt;h3 id=&quot;breaking&amp;#95;down&amp;#95;the&amp;#95;big&amp;#95;picture&quot;&gt;Breaking down the big picture&lt;/h3&gt;&lt;p&gt;The Geo-Data represents a number of plots, that is, subareas where the tree planting and treatment would be similar across all trees of the plot. In the simplest case, each plot is a list of latitude-longitude pairs that form a ring. All trees within the ring belong to the same plot. So given such Geo-Data we transform each &quot;ring of lat/long pairs&quot; to image coordinates and extract, using &lt;a href='http://gdal.org/'&gt;GDAL Library&lt;/a&gt;,  the corresponding subimage for each plot.  In our big file there were 35 plots and plots area ranged from 0.1 to 25 hectars, or a hundred to some 50,000 trees. Image sizes ranged similarly from less than a megabyte to a few tens of megabytes.&lt;/p&gt;&lt;p&gt;An added difficulty is that each plot might have holes in it (due to rocks, lakes or houses, ...) or even consist of several unconnected pieces (due to roads, rivers, ...).&lt;/p&gt;&lt;p&gt;So, assuming a numbering for each plot from 1 to 35, we process each unconnected piece and assign it a character. With all subimages named like so: 1a, 2a, 2b, 3a, ..., we know that we have to analyze images 2a and 2b separately and then just aggregate the results (= tree counts) to obtain the result for plot 2.&lt;/p&gt;&lt;p&gt;&lt;figure&gt;   &lt;img src=&quot;img/base-img00argb.jpg&quot; alt=&quot;A plot&quot;/&gt;   &lt;figcaption&gt;A single plot with approx. 6k trees&lt;/figcaption&gt; &lt;/figure&gt;&lt;/p&gt;&lt;h3 id=&quot;analyzing&amp;#95;an&amp;#95;image&quot;&gt;Analyzing an image&lt;/h3&gt;&lt;p&gt;This is where things get interesting: We have an image with three color channels and one transparency channel (red, green, blue, alpha) and we want to obtain the number of trees in the picture. We use the well-known &lt;a href='https://opencv.org/'&gt;OpenCV library&lt;/a&gt; for this and transform the image first to grayscale. &lt;/p&gt;&lt;p&gt;&lt;figure&gt;   &lt;img src=&quot;img/base-img00agray.jpg&quot; alt=&quot;A grayscale plot&quot;/&gt;   &lt;figcaption&gt;The same plot as above, but in grayscale&lt;/figcaption&gt; &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;Since the whole area of interest is somehow grayish (from 100 to 200 on a scale of 0-255) we map the image color space to &amp;#91;0,255&amp;#93;. So now the darkest pixels have grayscale 0 (= black) and the brightest have grayscale 255 (= white).&lt;/p&gt;&lt;p&gt;&lt;figure&gt;   &lt;img src=&quot;img/proc-img00agray.jpg&quot; alt=&quot;A grayscale plot with brightness rescaled&quot;/&gt;   &lt;figcaption&gt;The same plot as above, but in grayscale with brightness rescaled&lt;/figcaption&gt; &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;Then we first try counting the trees by using an algorithm that finds the brightest pixel relative to its neighbours, however, the result for an image with 6 thousand trees was some 30 thousand so that is obviously not the correct answer. The reason for this is noise: At the time the drone takes the big image, there might be dust particles or bird in the tree that affects the pixel brightness. We can remove that noise by smoothing out the image. This basically defines each pixel as the average of its own + its neighbours' pixel values. The problem here is of course that if we do this too much or too often, then our image becomes a grayish mass. Fortunately, since the tree canopies are up to 2m x 2m and the resolution is approx 10cm x 10cm, we can apply smoothing with a small kernel (which translates to a small neighbourhood of averaging the pixel values) without loosing track of the canopies or the dark regions between (= spaces between trees/canopies). &lt;/p&gt;&lt;p&gt;After smoothing we obtain a much better number that is already in the ball park of the number of trees. Still, it's generally too high (by approx 20%) so for each of the brightest pixels in the image we obtain the pixels around with slightly lower but approximately similar brightness. We can think of each of this as the pixel set that describes a tree canopy.&lt;/p&gt;&lt;p&gt;&lt;figure&gt;   &lt;img src=&quot;img/result-img00agray.jpg&quot; alt=&quot;The plot with areas of maximum brightness in blue&quot;/&gt;   &lt;figcaption&gt;The same plot as above, with areas of maximum brightness in blue. The result is still some 20% off.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/p&gt;&lt;p&gt;For each such pixel set we&lt;/p&gt;&lt;ul&gt;&lt;li&gt;split it into two if it's too long or wide - two very close canopies were identified as one&lt;/li&gt;&lt;li&gt;remove it if it's still just a single pixel - it can't be 10x10cm canopy so it must be noise&lt;/li&gt;&lt;li&gt;remove it if it's too dark compared to the surroundings - sometimes there are &quot;relatively bright&quot; pixels between tree canopies&lt;/li&gt;&lt;li&gt;remove it if it's at the edge of the area of interest - we keep the area of non-interest all black, so trivially there are a lof of relatively brightest pixels at the border.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;em&gt;To be continued...&lt;/em&gt;&lt;/p&gt;
</description>
<pubDate>
Tue, 03 Apr 2018 00:00:00 +0200
</pubDate>
</item>
<item>
<guid>
https://dmichulke.github.io/posts-output/2017-12-28-location-aware/
</guid>
<link>
https://dmichulke.github.io/posts-output/2017-12-28-location-aware/
</link>
<title>
Location-Aware Real-Time Marketing
</title>
<description>
&lt;p&gt;Location-Aware Marketing might be the next big thing in marketing.&lt;/p&gt;&lt;p&gt;After all, if still 90% of all purchases are done offline, then why not market in the offline segment as well? Competition is little and the potential impact on a company's bottomline is huge.&lt;/p&gt;&lt;p&gt;However, if it were that easy everybody would already be doing it. In what follows, I describe a business and technical overview of the challenges of such a system based on my experiences working as a consultant for almost 2 years in a Fortune 500 company where I was responsible for developing the prototype backend and putting it partly into production.&lt;/p&gt;&lt;p&gt;The project's goal was to market goods and services to retail customers via an established and widely used cell phone application for payments. Consequently, the app should be extended to allow for location-aware real-time marketing, i.e., &quot;promoting things when the customer is at a specific place at a specific time.&quot;&lt;/p&gt;&lt;p&gt;The resulting use cases were based on the cell phone acting as a location sensor and possibly coupon display under the following scenarios:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Customer enters a shopping mall -&gt; receives coupon to entice footfall in a new shop&lt;/li&gt;&lt;li&gt;Customer waits in his car at a traffic light -&gt; the ad billboard next to the traffic light is personalized to the customer's preferences&lt;/li&gt;&lt;li&gt;Customer leaves work around noon -&gt; offer lunch coupon to entice him and his colleagues to visit restaurant&lt;/li&gt;&lt;li&gt;Customer leaves work in the evening -&gt; offer grocery store coupon to entice him to do his grocery purchases there&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Besides the obvious cell phone application the system requires a back-end server that holds all coupon and customer data and responds fast even when being used by a few hundred thousands of people. My work was mostly on the back-end side, developing the database schema and the RESTful API while keeping an eye on application security (not so hard) and scalability (much harder). Here is a list of things to consider before you start developing your own coupon ad service:&lt;/p&gt;&lt;h3 id=&quot;technical&amp;#95;obstacle:&amp;#95;efficient&amp;#95;geo-lookup&quot;&gt;Technical Obstacle: Efficient Geo-Lookup&lt;/h3&gt;&lt;p&gt;The problem: Aggregating all coupons and serving those nearby, every few minutes. This must happen fast because at 1 million users and a polling interval of 5 minutes we need to serve 1600 request per seconds. &lt;/p&gt;&lt;p&gt;Proposed solution: Using &lt;a href='https://en.wikipedia.org/wiki/Hilbert_curve'&gt;Hilbert Curves&lt;/a&gt; and the &lt;a href='https://github.com/google/s2-geometry-library-java'&gt;google S2 library&lt;/a&gt;. It's also &lt;a href='www.metablake.com/foursquare/wsdm2013-final.pdf'&gt;Foursquare's approach&lt;/a&gt; and encodes location in databases via 64 bit integers. Since integers are easily indexed, fast lookups are straightforward and don't require any special extension (such as &lt;a href='http://postgis.net/'&gt;postgis&lt;/a&gt; at the cost of just a little bit more application logic. This makes the solution database-agnostic and indeed we switched from a postgres prototype to an oracle database without affecting the DB structure.  A good explanation of the underlying concepts can be found &lt;a href='http://blog.christianperone.com/2015/08/googles-s2-geometry-on-the-sphere-cells-and-hilbert-curve/'&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;h3 id=&quot;technical&amp;#95;obstacle:&amp;#95;recommendation&amp;#95;logic&quot;&gt;Technical Obstacle: Recommendation Logic&lt;/h3&gt;&lt;p&gt;For each request to serve nearby coupons we must order the coupons by relevance to the customer. This is to reduce cognitive load on the customer and to serve only the most important among the nearby coupons (reducing IO and bandwith usage thereby).&lt;/p&gt;&lt;p&gt;I chose to use a simple yet fast recommendation algorithm based on a &lt;a href='https://en.wikipedia.org/wiki/Naive_Bayes_classifier'&gt;Naive Bayesian Classifiers&lt;/a&gt;. Here, different dimensions &quot;constituting&quot; relevance are evaluated, each resulting in a probability. By multiplying these probabilities one arrives at an aggregate probability that the customer likes the coupon. &lt;/p&gt;&lt;p&gt;Examples for dimensions are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;coupon users within the last hours (&quot;hotness&quot; of the coupon)&lt;/li&gt;&lt;li&gt;coupon category (food, electronics, ...)&lt;/li&gt;&lt;li&gt;coupon tags (a set of keywords)&lt;/li&gt;&lt;li&gt;distance from customer to coupon&lt;/li&gt;&lt;li&gt;distance from typical customer locations (e.g., places where customer stays regularly for several hours per week) to coupon&lt;/li&gt;&lt;li&gt;time of day and day of week&lt;/li&gt;&lt;li&gt;customer coupon preferences (as inferred from the customer's interaction history)&lt;/li&gt;&lt;li&gt;customer eligibility (gender, age)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The approach has several advantages:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;It is simple and very fast to calculate and needs no extensive prior training, an ideal initial recommendation algorithm that can be gradually refined.&lt;/li&gt;&lt;li&gt;It is easy to use even if the customer has no history of interacting with coupons: You just use coupon hotness and distance, i.e., switch off most dimensions until you have enough data. You gradually turn those dimensions on when you have more data.&lt;/li&gt;&lt;li&gt;It is the opposite of a black box: You can easily explain why a given coupon is chosen over another and introduce weights to modify the importance of specific dimensions. Modifying those weights later on based on customer segment is straightforward&lt;/li&gt;&lt;/ol&gt;&lt;h3 id=&quot;business&amp;#95;obstacle:&amp;#95;defining&amp;#95;the&amp;#95;process&amp;#95;and&amp;#95;constraints&quot;&gt;Business Obstacle: Defining the Process and Constraints&lt;/h3&gt;&lt;p&gt;Regarding the process, many questions have to be answered:&lt;/p&gt;&lt;h4 id=&quot;creating&amp;#95;coupons&quot;&gt;Creating Coupons&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Who creates the coupons - the merchant via a web portal or a service desk employee? Is there some sort of double-check or revision mechanism in case some wrong data was entered?&lt;/li&gt;&lt;li&gt;Do you allow pictures with coupons? What if those contain adult material?&lt;/li&gt;&lt;li&gt;How do you interface with existing coupon campaign tools that your merchants might already use?&lt;/li&gt;&lt;/ul&gt;&lt;h4 id=&quot;using&amp;#95;coupons&quot;&gt;Using Coupons&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Can a coupon be &quot;reserved&quot; by a customer before actually being redeemed?&lt;/li&gt;&lt;li&gt;Are coupons transferrable?  Can people reserve coupons for others who otherwise wouldn't be eligible?&lt;/li&gt;&lt;li&gt;How do you identify yourself at the shop as the legitimate coupon owner?&lt;/li&gt;&lt;li&gt;Do you want public coupons or private coupons, public coupons being those that anyone can see/redeem and private coupons being those pushed only to specific people and only redeemable by those?&lt;/li&gt;&lt;/ul&gt;&lt;h4 id=&quot;coupon&amp;#95;constraints&quot;&gt;Coupon Constraints&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;(Maximum) Number of categories and tags?&lt;/li&gt;&lt;li&gt;Are there limits on how many coupons can be reserved or redeemed as a general cap?&lt;/li&gt;&lt;li&gt;Can a coupon be used more than once per person? How often? Are the limits applied to reservation or redemption?&lt;/li&gt;&lt;li&gt;Is reservation or redemption in any other way limited? Maybe only during week days, or only during specific hours? Or maybe only to specific locations of a merchant?&lt;/li&gt;&lt;li&gt;Until when coupons are valid?&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;business&amp;#95;obstacle:&amp;#95;business&amp;#95;case&amp;#95;and&amp;#95;actions&quot;&gt;Business Obstacle: Business Case and Actions&lt;/h3&gt;&lt;p&gt;Depending on your business model, you will need infrastructure in place:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;If you sell impressions: You will need to control the content of ad screens and (trivially) the app&lt;/li&gt;&lt;li&gt;If you sell footfalls: Your app will need to be able to connect to the internet within the target shop or at least get GPS coordinates for later use. Correcting for people who only pass by shouldn't be a big problem given enough data.&lt;/li&gt;&lt;li&gt;If you sell redemptions: This is probably only easy if you control the payment network or the Merchant's Point of Sale.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;business&amp;#95;obstacle:&amp;#95;promoting&amp;#95;the&amp;#95;app&quot;&gt;Business Obstacle: Promoting the app&lt;/h3&gt;&lt;p&gt;Finally, you need to get the app on enough consumer cell phones in order to be able to attract merchants.  It's probably easiest and cheapest to start in a big city and focus on growing merchants and consumers there. Once you have a name there, you can expand to other cities.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;You have a similar project in mind and need help?&lt;/em&gt; &lt;em&gt;&lt;a href='/pages-output/contact/'&gt;Let me know&lt;/a&gt;!&lt;/em&gt;&lt;/p&gt;
</description>
<pubDate>
Thu, 28 Dec 2017 00:00:00 +0100
</pubDate>
</item>
</channel>
</rss>
